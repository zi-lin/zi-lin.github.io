---
title: "Semantic Role Labeling"
hidden: true
last_modified_at:
tag: NLP Task
date: 2018-11-10
---
# Introduction

**Semantic Role Labeling (SRL)**, sometimes called *shallow semantic parsing*, is a process in natrual language processing that assigns semantic roles to constituents or their head words in a sentence according to their relationship to the predicates expressed in the sentence. Typical semantic roles can be divided into core **arguments** and **adjuncts**. The core arguments include *Agent, Patient, Source, Goal*, etc, While the adjunts include *Location, Time, Manner, Cause*, etc.

For example, in the sentence ``I ate breakfast quickly in the car this morning because I was in a hurry``, we can have the labeled sentence like this:

> \[I\]<sub>agent</sub> \[**eat**\]<sub>predicate</sub> \[breakfast\]<sub>patient</sub> \[quickly\]<sub>manner</sub> \[in the car\]<sub>location</sub> \[this morning\]<sub>time</sub> \[because I was in a hurry\]<sub>cause</sub> .

# Source
There are mainly three sources for SRL, namely, [PropBank](https://propbank.github.io/), [FrameNet](https://framenet.icsi.berkeley.edu/fndrupal/) and [VerbNet](https://verbs.colorado.edu/~mpalmer/projects/verbnet.html).

## PropBank
PropBank is a corpus in which the arguments of each predicate are annotated with their semantic roles in relation to the predicate [^1]. Currently, all the PropBank annotations are done on top of the phrase structure annotation of the **Pen TreeBank** [^2]. In addition to semantic role annotation, PropBank annotation requires the choice of a sense ID (also known as *frameset* or *roleset* ID) for each predicate. Thus, for each verb in every tree (representing the phrase structure of the corresponding sentence), PropBank has the instance that consists of the sense ID of the predicate (e.g. eat.01) and its arguments labeled with semantic roles.

An important goal is to provide consistent argument labels across different syntactic realizations of the same verb, as in...

> \[I\]<sub>ARG0</sub> \[**eat**\]<sub>predicate</sub> \[breakfast\]<sub>ARG1</sub> \[quickly\]<sub>ARGM-MNR</sub> \[in the car\]<sub>ARGM-LOC</sub> \[this morning\]<sub>ARGM-TMP</sub> \[because I was in a hurry\]<sub>ARGM-CAU</sub> .

The argument structure of each predicate is outlined in the PropBank frame file for that predicate. The frame file denotes the correspondences between numbered arguments and semantic roles, as this is somewhat unique for each predicate. Numbered arguments reflect either the arguments that are requied for the valency of a predicate (e.g., agent, patient, benefactive), or if not requied, those that occur with high-frequency in actual usage. Although numbered arguments correspond to slightly different semantic roles given the usage of each predicate, in general numbered arguments correspond to the following semantic roles:

| **ARG0** |                agent               | **ARG3** | starting point, benefactive, attribute |
| **ARG1** |               patient              | **ARG4** |              ending point              |
| **ARG2** | instrument, benefactive, attribute | **ARGM** |                modifier                |

# Data

- **CoNLL-2005:** The CoNLL-2005 dataset takes section 2-21 of the Wall Street Journal (WSJ) corpus as training set, and section 24 as development set. The test set consist of section 23 of the **WSJ** corpus (in-domain test) as well as 3 section from the **Brown** corpus (out-of-domain test) [^3].

- **CoNLL-2012:** The CoNLL-2012 dataset is extracted from the OntoNotes v5.0 corpus [^4]. The description and separation of training, development and test set can be found in [Pardhan et al. (2013)](http://www.aclweb.org/anthology/W13-3516).

# Experiments & Results

Note: All the statistics are obtained from the exact papers or papers citing the excact papers. Contact me if the statistics are not correct or you think your model should be listed as one of them.

## With Gold Predicate

Of course, for each model, we can have different versions, e.g., using the word embeddings from Glove or ELMo, and different versions can lead to different results. Here I just list the results for one of the versions, most of which are single models without using any tricks. I will specify if there are tricks leveraged, e.g., LISA+**D&M**. You should look the original paper if you are intersted in the model, and check whether there are some other versions of the model yielding better results.

| Paper | Model | WSJ | Brown | CoNLL-2012 |
|:-----:|:-----:|:---:|:-----:|:----------:|
| [EMNLP-18](http://aclweb.org/anthology/D18-1548) | LISA+D&M<br>*University of Massachusetts Amherst & Google AI Language*|  **86.04** | **76.54** | - |
| [NAACL-18](http://aclweb.org/anthology/N18-1202) | Peter et al. (2018)+ELMo<br>*Allen Institute for Artificial Intelligence & University of Washington* | - | - | **84.6** |
| [AAAI-18](https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/download/16725/16025) | Tan et al. (2018)<br>*Xiamen University & Tencent Technology Co.* | 84.8 | 74.1 | 82.7 | 
| [ACL-18](http://aclweb.org/anthology/P18-2058) | He et al. (2018)<br>*University of Washington* | 83.9 | 73.7 | 82.1 |
| [ACL-17](http://aclweb.org/anthology/P17-1044) | He et al. (2017)<br>*University of Wahsington, Facebook AI Research & Allen Institute for Artificial Intelligence*| 83.1 | 72.1 | 81.7 |
| [EMNLP-17](http://aclweb.org/anthology/D17-1128) | Yang and Mitchell (2017)<br>*Carnegie Mellon University*| 81.9 | 72.0 | - |
| [ACL-15](http://www.aclweb.org/anthology/P15-1109) | Zhou and Xu (2015)<br>*Baidu Research* | 82.8 | 69.4 | 81.1 |
| [EMNLP-15](http://aclweb.org/anthology/D15-1112) | FitzGerald at al. (2015)<br>*University of Washington & Google* | 80.3 | 72.2 | 80.1 |
| [TACL-15](http://aclweb.org/anthology/Q15-1003) | Tackstrom et al. (2015)<br>*Google* | 79.9 | 71.3 | 79.4 |
| [CoNLL-13](http://www.aclweb.org/anthology/W13-3516) | Pradhan et al. (2013)<br>*Boston Childrens Hospital and Harvard Medical School, University of Trento, QCRI, Brandeis University, National University of Singapore & University of Stuttgart*   |   -  |   -  | 77.5 |
| [CL-08](https://www.aclweb.org/anthology/J/J08/J08-2002.pdf) | Toutanova et al. (2008)<br>*Microsoft Research, University of California berkeley & Stanford University* | 80.3 | 68.8 |  -   |
| [CL-08](https://www.mitpressjournals.org/doi/pdf/10.1162/coli.2008.34.2.257) | Punyakanok et al. (2008)<br>*BBN Technologies, University of Illinois at Urbana-Champaign & Microsoft Research*| 79.4 | 67.8 |  -   | 


## Without Gold Predicate

| Paper | Model | WSJ | Brown | CoNLL-2012 |
|:-----:|:-----:|:---:|:-----:|:----------:|
| [EMNLP-18](http://aclweb.org/anthology/D18-1548) | LISA+ELMo+D&M<br>*University of Massachusetts Amherst & Google AI Language* |  **86.90** | **78.25** | **83.38** |
| [ACL-18](http://aclweb.org/anthology/P18-2058) | He et al. (2018) ELMo<br>*University of Washington* | 86.0 | 76.1 | 82.9 |
| [ACL-17](http://aclweb.org/anthology/P17-1044) | He et al. (2017) GloVe+PoE<br>*University of Wahsington, Facebook AI Research & Allen Institute for Artificial Intelligence* | 82.7 | 70.1 | 78.4 |

[^1]: Martha Palmer, Daniel Gildea, and Paul Kingsbury. 2005. The proposition bank: An annotated corpus of semantic roles. Computational Linguistics, 31(1):71-106.
[^2]: Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. 1993. Building a large annotated corpus of english: the penn treebank. Computational Linguistics, 19(2):313{330.
[^3]: Xavier Carreras and Lluis Marquez. 2005. Introduction to the conll-2005 shared task: Semantic role labeling. In CoNLL.
[^4]: Sameer Pradhan, Alessandro Moschitti, Nianwen Xue, Hwee Tou Ng, Anders Bjorkelund, Olga Uryupina, Yuchen Zhang, and Zhi Zhong. 2013. Towards robust linguistic analysis using ontonotes. In CoNLL.






















