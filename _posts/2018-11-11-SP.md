---
title: "Semantic Parsing"
hidden: true
last_modified_at: 2018-11-26
tag: NLP Tasks
author: Zi Lin
date: 2018-11-11
---
# Introduction
In fact, "semantic parsing" is, ironically, a semantically ambiguous term, which could refer to:

- Semantic role labeling (also known as shallow semantic parsing, see previous post for more information).
- Finding generic relations in text.
- Transforming a natrual language sentence into its meaning representation.

In this post, semantic parsing is the task of mapping natrual language sentences (NL) into complete **formal meaning representation** (MRs) which a computer can execute for some domain -specific application [^1]. Representing the meaning of natural language is ultimately a difficult philosophical question and many attempts have been made to define generic formal semantics of natural language, including:

- **Meaning Representation Language (MRL):** MRL is designed by the creator of the application to suit the application's needs independent of natural language. Some MRL data sets are based on Prolog database.

- **SQL Query**

- **Abstract Meaning Representation (AMR)**

- **Minimal Recursion Semantics (MRS)**

- **Semantic Dependency Parsing (SDP)**

# Abstract Meaning Representation

Abstract Meaning Representation (AMR) [^2] is a semantic formalism in which the meaning of a sentence is encoded as a rooted, directed, acyclic graph. Nodes represent concepts, and labeled directed graph edges represent the relationships between them. For example, for the sentence "the boy wants to believe the girl" we can have the corresponding AMR graph (in penman format) as:

```
(w / want-01
	:ARG0 (b / boy)
	:ARG1 (b2 / believe-01
		:ARG0 (g / girl)
		:ARG1 b))
```

AMR parsing, the task of transforming a sentence into its AMR graph, is a challenging task as it requires the parser to learn to predict not only concepts, which consist of **predicates**, **lemmas**, **named entites**, **wiki-links** and **co-references**, but also a large number of **relation types** based on the semantic role in PropBank (see the previous blog [Semantic Role Labeling](../SRL/) if you are interested).

## Data
There are multiple version of AMR data, including:
- **LDC2013E117:** 10,853 sentences with 13,050 AMRs (corpus includes multiple annotations for some sentences)
- **[LDC2014T12](https://catalog.ldc.upenn.edu/LDC2014T12) (AMR 1.0):** 13,051 sentences.
- **LDC2015E86:** 19,572 sentences. This version includes more AMRs, wikification, AMR adoption of new unfied PropBank frames, AMR deepening, automatic AMR-English alignments, and correction of AMR annotation errors.
- **LDC2016E25:** 39,260 sentences.
- **[LDC2017T10](https://catalog.ldc.upenn.edu/LDC2017T10) (AMR 2.0):** 39,260 sentences.

## Experiment & Results

These are the AMR parsing results on LDC2014T12, LDC2015E86 and LDC2017T10 test sets, where 2014N refers to the newswire section of LDC2014T12. You should look the original paper if you are intersted in the model. **Please contact me (through email) if the statistics is incorrect.**

| Paper | Model | LDC2014N | LDC2014 | LDC2015 | LDC2017 |
|:-----:|:-----:|:--------:|:-------:|:-------:|:-------:|
|[ACL-18](http://aclweb.org/anthology/P18-1037) | Lyu & Titov<br>*University of Edinburgh & University of Amsterdam* | - | - | **73.7** | **74.4** |
|[ACL-18](http://aclweb.org/anthology/P18-1170) | Groschwitz et al.<br>*Saarland University & Macquerie University* | - | - | 70.2 | 71.0 |
|[ACL-17](http://www.aclweb.org/anthology/P17-1112.pdf) | Buys & Blunsom<br>*University of Oxford & Deepmind* | - | - | - | 61.9 |
|[ACL-17](http://www.aclweb.org/anthology/P17-1043) | Foland & Martin<br>*University of Colorado* | - | - | 70.7 | - |
|[EMNLP-18](http://aclweb.org/anthology/D18-1198) | Guo & Lu<br>*Singapore University of Technology & Design* | **74.0** | **68.3** | 68.7 | 69.8 |
|[Arxiv-17](https://arxiv.org/abs/1705.09980) | van Noord & Bos<br>*University of Groningen* | - | - | 68.5 | 71.0 | 
|[EMNLP-17](http://aclweb.org/anthology/D17-1129) | Wang & Xue<br>*Brandeis University* | - | 68.1 | 68.1 | - |
|[Semeval-16](http://www.aclweb.org/anthology/S16-1181) | Wang et al. (CAMR)<br>*Brandeis University & Boulder Language Technologies* | - | 66.5 | 67.3 | - |
|[Semeval-16](http://www.aclweb.org/anthology/S16-1176) | Barzdins & Gosko<br>*University of Latvia* | - | - | 67.2 | - |
|[Semeval-16](http://aclweb.org/anthology/S16-1186) | Flanigan et al. (JAMR)<br>*Carnegie Mellon University & University of Washington* | -  | 66 | 67 | - |
|[EACL-17](http://www.aclweb.org/anthology/E17-1051) | Damonte et al.<br>*University of Edinburgh & University of Padua* | - | 64 | 64 | - |
|[NAACL-18](http://aclweb.org/anthology/N18-2023) | Vilares & Gómez-Rodríguez<br>*Universidade da Coruña* | - | - | 64 | - |
|[AAAI-18](https://aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16563/16021) | Peng et al.<br>*University of Rochester & University of Padua* | - | - | 64 | - |
|[ACL-17](http://aclweb.org/anthology/P17-1014) | Konstas et al.<br>*University of Washington & Allen Institute for Artificial Intelligence* | - | - | 62.1 | - |
|[EACL-17](http://aclweb.org/anthology/E17-1035) | Peng et al.<br>*University of Rochester & Brandeis University* | - | - | 52 | - |
|[EMNLP-15](http://www.aclweb.org/anthology/D15-1136) | Pust et al.<br>*Univerisity of Southern California* | - | 67.1 | - | - |
|[EMNLP-16](http://aclweb.org/anthology/D16-1065) | Zhou et al.<br>*Nianjin Normal University & DFKI, Germany* | 71 | 66 | - | - |
|[Semeval-16](http://www.aclweb.org/anthology/S16-1180) | Goodman et al.<br>*University College London & University of Sheffeld* | 70 | - | 64 | - |
|[ACL-IJCNLP-15](http://www.aclweb.org/anthology/P15-2141.pdf) | Wang et al. (CAMR)<br>*Brandeis University & Boulder Language Technologies* | 70 | 66 | - | - |
|[EMNLP-17](http://aclweb.org/anthology/D17-1130) | Ballesteros & AI-Onaizan<br>*IBM T.J Waston Research Center* | 69 | 64 | - | - |
|[EMNLP-15](http://www.aclweb.org/anthology/D15-1198) | Artzi et al.<br>*Cornell University & University of Washington* | 67 | - | - | - |
|[ACL-IJCNLP-15](http://aclweb.org/anthology/P15-1095) | Werling et al.<br>*Stanford University* | 62 | - | - | - |
|[NAACL-15](http://www.aclweb.org/anthology/N15-1040) | Wang et al. (CAMR)<br>*Brandeis University & Harvard Medical School* | - | 59 | - | - |
|[ACL-14](http://aclweb.org/anthology/P14-1134) | Flanigan et al. (JAMR)<br>*Carnegie Mellon University* | 59 | 58 | - | - |
|[CoNLL-15](https://www.aclweb.org/anthology/K15-1004) | Peng et al.<br>*University of Rochester* | 58 | - | - | - |

# Semantic Dependency Parsing

Task 8 at SemEval 2014 defines *Broad-Coverage Semantic Dependency Parsing* (SDP) as the problem of recovering sentence-internal predicate-argument relationships for *all content words*, i.e., the semantic structure constituting the relational core of sentence meaning [^3].

SDP seeks to stimulate the dependency parsing community to move towards more general graph processing, to thus enable a more direct analysis of *Who did What to Whom?* In additon to its relation to syntactic dependency parsing, the task also has some overlap with Semantic Role Labeling (SRL) [^4].

The SDP parsers are required to identify all semantic dependencies, i.e., compute a representation that integrates all content words in one structure. Different from SRL, SDP task does not encompass predicate disambiguation, a design decision in part owed to SDP's goal to focus on parsing-oriented, i.e., structural, analysis, and in part to lacking consensus on sense inventories for all content words.

SDP uses three distinct targer representations for semantic dependencies. Including:

## DM: DELPH-IN MRS-Derived Bi-Lexical Dependencies

These semantic dependency graphs originate in a manual re-annotation of Sections 00–21 of the WSJ Corpus with syntactico-semantic analyses derived from the LinGO English Resource Grammar (ERG) [^5]. The DM targer representations are derived through a two-step 'lossy' conversion of MRSs, first to variable-free Elementary Dependency Structures (EDS) [^6], then to 'pure' bi-lexical form - projecting some construction semantics onto word-to-word dependencies [^7]. For example, for the sentence "Ms. Haag plays Elianti.", we have the DM representation as follows:

| id | form | lemma | pos | top | pred | arg1 | arg2 |
|:--:|:----:|:-----:|:---:|:---:|:----:|:----:|:----:|
| 1 | Ms. | Ms. | NNP | - | + | \_ | \_ |
| 2 | Haag| Haag| NNP | - | - | compound | ARG1|
| 3 | plays|play| VBZ | + | + | \_ | \_ |
| 4 |Elianti|Elianti|NNP|-| - | \_ | ARG2|
| 5 | .     | . | . |-| - | \_ | \_ |

## PAS: Enju Predicate-Argument Structures

The Enju parsing system is an HPSG-based parser for English. The grammar and the dismbiguation model of this parser are derived from the Enju HPSG treebank, which is automatically converted from the phrase structure and predicate-argument structure annotation of the PTB. The PAS data set is extracted from the WSJ portion of the Enju HPSG treebank. While the Enju treebank is annotated with full HPSG-style structures, only its predicate-argument strcutres are converte into the SDP data format for use in the SDP task.

## PCEDT: Prague Tectogrammatical Bi-Lexical Dependencies

The Prague Czech-English Dependency TreeBank (PCEDT) [^8] is a set of parallel dependency trees over the WSJ texts from the PTB, and their Czech translations. The specifics of the PCEDT representations are best observed in the procedure that converts the original PCEDT data to the SDP data format; see Miyao et al. 2014 [^9].

# Data
- **[Semeval-2014 Task 8](http://sdp.delph-in.net/2014/data.html):** For this task, there will be three data sets: training, development, and test.  On November 4, 2013, a sample (of some 190 dependency graphs) from the training data was published as [trial data](http://svn.delph-in.net/sdp/public/2014/trial/current.tgz), demonstrating key characteristics of the task.  Since December 13, some 750,000 tokens of annotated text are available as training data; please subscribe to the task [mailing list](http://lists.emmtee.net/mailman/listinfo/sdp-participants) for access information.
- **[Semeval-2015 Task 8](http://sdp.delph-in.net/2015/data.html):** This Task is a re-run with some extensions of Task 8 at SemEval-2014. 

# Experiment & Results

The results are the labeled parsing performance (F1 score) on both in-domain and out-of-domain test data and are based on the English dataset from SemEval 2015 Task 18 closed track. The data split is 33,964 training sentences from 00-19 of the WSJ corpus, 1,692 development sentences from 20, 1,410 sentences from 21 as in domain test data, and 1,849 sentences sampled from the Brown Corpus as out-of-domain test data.

## In-Domain

| paper | model | DM | PAS | PSD |
|:-----:|:-----:|:--:|:---:|:---:|
|[ACL-2018](http://aclweb.org/anthology/P18-2077)|Dozat & Manning<br>*Stanford University*|**93.7**|**93.9**|**81.0**|
|[ACL-2017](http://aclweb.org/anthology/P17-1186)|Peng et al.<br>*University of Washington & Carnegie Mellon University*|90.4|92.7|78.5|
|[AAAI-2018](https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16549/16113)|Wang et al.<br>*Harbin Institute of Technology*|90.3|91.7|78.6|
|[Semeval-2015](http://aclweb.org/anthology/S15-2154)|Du et al.<br>*Peking University*|89.1|91.3|75.7|
|[Semeval-2015](http://aclweb.org/anthology/S15-2162)|Almeida & Martins<br>*Priberam Labs & Instituto de Telecomunicacoes*|88.2|90.9|76.4|

## Out-Of-Domain

| paper | model | DM | PAS | PSD |
|:-----:|:-----:|:--:|:---:|:---:|
|[ACL-2018](http://aclweb.org/anthology/P18-2077)|Dozat & Manning<br>*Stanford University*|**88.9**|**90.6**|**79.4**|
|[ACL-2017](http://aclweb.org/anthology/P17-1186)|Peng et al.<br>*University of Washington & Carnegie Mellon University*|85.3|89.0|76.4|
|[AAAI-2018](https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16549/16113)|Wang et al.<br>*Harbin Institute of Technology*|84.9|87.6|75.9|
|[Semeval-2015](http://aclweb.org/anthology/S15-2154)|Du et al.<br>*Peking University*|81.8|87.2|73.3|
|[Semeval-2015](http://aclweb.org/anthology/S15-2162)|Almeida & Martins<br>*Priberam Labs & Instituto de Telecomunicacoes*|81.8|86.9|74.8|

[^1]: Rohit J Kate and YukWahWong. 2010. Semantic parsing: The task, the state of the art and the future. Tutorial Abstracts of ACL 2010 page 6.
[^2]: Laura Banarescu, Claire Bonial, Shu Cai, Madalina Georgescu, Kira Griffitt, Ulf Hermjakob, Kevin Knight, Philipp Koehn, Martha Palmer, and Nathan Schneider. 2013. Abstract meaning representation for SemBanking. In Proc. of the 7th Linguistic Annotation Workshop and Interoperability with Discourse.
[^3]: Oepen, S., Kuhlmann, M., Miyao, Y., Zeman, D., Flickinger, D., Hajic, J., Ivanova, Angelina, Zhang, Y. 2014. SemEval 2014 Task 8: Broad-coverage semantic dependency parsing. In Proceedings of the 8th International Workshop on Semantic Evaluation. Dublin, Ireland.
[^4]: Gildea, D., & Jurafsky, D. 2002. Automatic labeling of semantic roles. Computational Linguistics, 28, 71:245–288.
[^5]: Flickinger, D., Zhang, Y., & Kordoni, V. 2012. DeepBank. A dynamically annotated treebank of the Wall Street Journal. In Proceedings of the 11th International Workshop on Treebanks and Linguistic Theories (p. 85–96). Lisbon, Portugal: Edições Colibri.
[^6]: Oepen, S., & Lønning, J. T. 2006. Discriminantbased MRS banking. In Proceedings of the 5th International Conference on Language Resources and Evaluation (p. 1250–1255). Genoa, Italy.
[^7]: Ivanova, A., Oepen, S., Øvrelid, L., & Flickinger, D. 2012. Who did what to whom? A contrastive study of syntacto-semantic dependencies. In Proceedings of the Sixth Linguistic Annotation Workshop (p. 2–11). Jeju, Republic of Korea.
[^8]: Hajic, J., Hajicová, E., Panevová, J., Sgall, P., Bojar, O., Cinková, S., ... & Semecký, J. 2012. Announcing Prague Czech-English Dependency Treebank 2.0. In LREC (pp. 3153-3160).
[^9]: Miyao, Y., Oepen, S., & Zeman, D. (2014). In-house: An ensemble of pre-existing off-the-shelf parsers. In Proceedings of the 8th International Workshop on Semantic Evaluation. Dublin, Ireland.
