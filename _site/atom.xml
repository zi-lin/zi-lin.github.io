<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.8.5">Jekyll</generator><link href="http://localhost:4000/atom.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2018-11-10T19:16:38+08:00</updated><id>http://localhost:4000/</id><title type="html">Zi Lin</title><subtitle>[firstname].[lastname]@pku.edu.cn</subtitle><author><name>Zi Lin</name></author><entry><title type="html">Semantic Role Labeling</title><link href="http://localhost:4000/SRL/" rel="alternate" type="text/html" title="Semantic Role Labeling" /><published>2018-11-10T00:00:00+08:00</published><updated>2018-11-10T00:00:00+08:00</updated><id>http://localhost:4000/SRL</id><content type="html" xml:base="http://localhost:4000/SRL/">&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;Semantic Role Labeling (SRL)&lt;/strong&gt;, sometimes called &lt;em&gt;shallow semantic parsing&lt;/em&gt;, is a process in natrual language processing that assigns semantic roles to constituents or their head words in a sentence according to their relationship to the predicates expressed in the sentence. Typical semantic roles can be divided into core &lt;strong&gt;arguments&lt;/strong&gt; and &lt;strong&gt;adjuncts&lt;/strong&gt;. The core arguments include &lt;em&gt;Agent, Patient, Source, Goal&lt;/em&gt;, etc, While the adjunts include &lt;em&gt;Location, Time, Manner, Cause&lt;/em&gt;, etc.&lt;/p&gt;

&lt;p&gt;For example, in the sentence &lt;code class=&quot;highlighter-rouge&quot;&gt;I ate breakfast quickly in the car this morning because I was in a hurry&lt;/code&gt;, we can have the labeled sentence like this:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;[I]&lt;sub&gt;agent&lt;/sub&gt; [&lt;strong&gt;eat&lt;/strong&gt;]&lt;sub&gt;predicate&lt;/sub&gt; [breakfast]&lt;sub&gt;patient&lt;/sub&gt; [quickly]&lt;sub&gt;manner&lt;/sub&gt; [in the car]&lt;sub&gt;location&lt;/sub&gt; [this morning]&lt;sub&gt;time&lt;/sub&gt; [because I was in a hurry]&lt;sub&gt;cause&lt;/sub&gt; .&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h1 id=&quot;source&quot;&gt;Source&lt;/h1&gt;
&lt;p&gt;There are mainly three sources for SRL, namely, &lt;a href=&quot;https://propbank.github.io/&quot;&gt;PropBank&lt;/a&gt;, &lt;a href=&quot;https://framenet.icsi.berkeley.edu/fndrupal/&quot;&gt;FrameNet&lt;/a&gt; and &lt;a href=&quot;https://verbs.colorado.edu/~mpalmer/projects/verbnet.html&quot;&gt;VerbNet&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;propbank&quot;&gt;PropBank&lt;/h2&gt;
&lt;p&gt;PropBank is a corpus in which the arguments of each predicate are annotated with their semantic roles in relation to the predicate &lt;sup id=&quot;fnref:1&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;. Currently, all the PropBank annotations are done on top of the phrase structure annotation of the &lt;strong&gt;Pen TreeBank&lt;/strong&gt; &lt;sup id=&quot;fnref:2&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;. In addition to semantic role annotation, PropBank annotation requires the choice of a sense ID (also known as &lt;em&gt;frameset&lt;/em&gt; or &lt;em&gt;roleset&lt;/em&gt; ID) for each predicate. Thus, for each verb in every tree (representing the phrase structure of the corresponding sentence), PropBank has the instance that consists of the sense ID of the predicate (e.g. eat.01) and its arguments labeled with semantic roles.&lt;/p&gt;

&lt;p&gt;An important goal is to provide consistent argument labels across different syntactic realizations of the same verb, as in…&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;[I]&lt;sub&gt;ARG0&lt;/sub&gt; [&lt;strong&gt;eat&lt;/strong&gt;]&lt;sub&gt;predicate&lt;/sub&gt; [breakfast]&lt;sub&gt;ARG1&lt;/sub&gt; [quickly]&lt;sub&gt;ARGM-MNR&lt;/sub&gt; [in the car]&lt;sub&gt;ARGM-LOC&lt;/sub&gt; [this morning]&lt;sub&gt;ARGM-TMP&lt;/sub&gt; [because I was in a hurry]&lt;sub&gt;ARGM-CAU&lt;/sub&gt; .&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The argument structure of each predicate is outlined in the PropBank frame file for that predicate. The frame file denotes the correspondences between numbered arguments and semantic roles, as this is somewhat unique for each predicate. Numbered arguments reflect either the arguments that are requied for the valency of a predicate (e.g., agent, patient, benefactive), or if not requied, those that occur with high-frequency in actual usage. Although numbered arguments correspond to slightly different semantic roles given the usage of each predicate, in general numbered arguments correspond to the following semantic roles:&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;ARG0&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;agent&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;ARG3&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;starting point, benefactive, attribute&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;ARG1&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;patient&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;ARG4&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;ending point&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;ARG2&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;instrument, benefactive, attribute&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;ARGM&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;modifier&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h1 id=&quot;data&quot;&gt;Data&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;&lt;a href=&quot;http://www.lsi.upc.edu/~srlconll/conll05st-release.tar.gz&quot;&gt;CoNLL-2005&lt;/a&gt;:&lt;/strong&gt; The CoNLL-2005 dataset takes section 2-21 of the Wall Street Journal (WSJ) corpus as training set, and section 24 as development set. The test set consist of section 23 of the &lt;strong&gt;WSJ&lt;/strong&gt; corpus (in-domain test) as well as 3 section from the &lt;strong&gt;Brown&lt;/strong&gt; corpus (out-of-domain test) &lt;sup id=&quot;fnref:3&quot;&gt;&lt;a href=&quot;#fn:3&quot; class=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;&lt;a href=&quot;http://conll.cemantix.org/2012/data.html&quot;&gt;CoNLL-2012&lt;/a&gt;:&lt;/strong&gt; The CoNLL-2012 dataset is extracted from the OntoNotes v5.0 corpus &lt;sup id=&quot;fnref:4&quot;&gt;&lt;a href=&quot;#fn:4&quot; class=&quot;footnote&quot;&gt;4&lt;/a&gt;&lt;/sup&gt;. The description and separation of training, development and test set can be found in &lt;a href=&quot;http://www.aclweb.org/anthology/W13-3516&quot;&gt;Pardhan et al. (2013)&lt;/a&gt;.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;experiments--results&quot;&gt;Experiments &amp;amp; Results&lt;/h1&gt;

&lt;p&gt;Note: All the statistics are obtained from the exact papers or papers citing the excact papers. Contact me if the statistics are not correct or you think your model should be listed as one of them.&lt;/p&gt;

&lt;h2 id=&quot;with-gold-predicate&quot;&gt;With Gold Predicate&lt;/h2&gt;

&lt;p&gt;Of course, for each model, we can have different versions, e.g., using the word embeddings from Glove or ELMo, and different versions can lead to different results. Here I just list the results for one of the versions, most of which are single models without using any tricks. I will specify if there are tricks leveraged, e.g., LISA+&lt;strong&gt;D&amp;amp;M&lt;/strong&gt;. You should look the original paper if you are intersted in the model, and check whether there are some other versions of the model yielding better results.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Paper&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Model&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;WSJ&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Brown&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;CoNLL-2012&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;a href=&quot;http://aclweb.org/anthology/D18-1548&quot;&gt;EMNLP-18&lt;/a&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;LISA+D&amp;amp;M&lt;br /&gt;&lt;em&gt;University of Massachusetts Amherst &amp;amp; Google AI Language&lt;/em&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;strong&gt;86.04&lt;/strong&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;strong&gt;76.54&lt;/strong&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;-&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;a href=&quot;http://aclweb.org/anthology/N18-1202&quot;&gt;NAACL-18&lt;/a&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Peter et al. (2018)+ELMo&lt;br /&gt;&lt;em&gt;Allen Institute for Artificial Intelligence &amp;amp; University of Washington&lt;/em&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;-&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;-&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;strong&gt;84.6&lt;/strong&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;a href=&quot;https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/download/16725/16025&quot;&gt;AAAI-18&lt;/a&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Tan et al. (2018)&lt;br /&gt;&lt;em&gt;Xiamen University &amp;amp; Tencent Technology Co.&lt;/em&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;84.8&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;74.1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;82.7&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;a href=&quot;http://aclweb.org/anthology/P18-2058&quot;&gt;ACL-18&lt;/a&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;He et al. (2018)&lt;br /&gt;&lt;em&gt;University of Washington&lt;/em&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;83.9&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;73.7&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;82.1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;a href=&quot;http://aclweb.org/anthology/P17-1044&quot;&gt;ACL-17&lt;/a&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;He et al. (2017)&lt;br /&gt;&lt;em&gt;University of Wahsington, Facebook AI Research &amp;amp; Allen Institute for Artificial Intelligence&lt;/em&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;83.1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;72.1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;81.7&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;a href=&quot;http://aclweb.org/anthology/D17-1128&quot;&gt;EMNLP-17&lt;/a&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Yang and Mitchell (2017)&lt;br /&gt;&lt;em&gt;Carnegie Mellon University&lt;/em&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;81.9&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;72.0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;-&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;a href=&quot;http://www.aclweb.org/anthology/P15-1109&quot;&gt;ACL-15&lt;/a&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Zhou and Xu (2015)&lt;br /&gt;&lt;em&gt;Baidu Research&lt;/em&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;82.8&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;69.4&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;81.1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;a href=&quot;http://aclweb.org/anthology/D15-1112&quot;&gt;EMNLP-15&lt;/a&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;FitzGerald at al. (2015)&lt;br /&gt;&lt;em&gt;University of Washington &amp;amp; Google&lt;/em&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;80.3&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;72.2&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;80.1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;a href=&quot;http://aclweb.org/anthology/Q15-1003&quot;&gt;TACL-15&lt;/a&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Tackstrom et al. (2015)&lt;br /&gt;&lt;em&gt;Google&lt;/em&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;79.9&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;71.3&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;79.4&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;a href=&quot;http://www.aclweb.org/anthology/W13-3516&quot;&gt;CoNLL-13&lt;/a&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Pradhan et al. (2013)&lt;br /&gt;&lt;em&gt;Boston Childrens Hospital and Harvard Medical School, University of Trento, QCRI, Brandeis University, National University of Singapore &amp;amp; University of Stuttgart&lt;/em&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;-&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;-&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;77.5&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;a href=&quot;https://www.aclweb.org/anthology/J/J08/J08-2002.pdf&quot;&gt;CL-08&lt;/a&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Toutanova et al. (2008)&lt;br /&gt;&lt;em&gt;Microsoft Research, University of California berkeley &amp;amp; Stanford University&lt;/em&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;80.3&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;68.8&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;-&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;a href=&quot;https://www.mitpressjournals.org/doi/pdf/10.1162/coli.2008.34.2.257&quot;&gt;CL-08&lt;/a&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Punyakanok et al. (2008)&lt;br /&gt;&lt;em&gt;BBN Technologies, University of Illinois at Urbana-Champaign &amp;amp; Microsoft Research&lt;/em&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;79.4&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;67.8&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;-&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;without-gold-predicate&quot;&gt;Without Gold Predicate&lt;/h2&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Paper&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Model&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;WSJ&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Brown&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;CoNLL-2012&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;a href=&quot;http://aclweb.org/anthology/D18-1548&quot;&gt;EMNLP-18&lt;/a&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;LISA+ELMo+D&amp;amp;M&lt;br /&gt;&lt;em&gt;University of Massachusetts Amherst &amp;amp; Google AI Language&lt;/em&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;strong&gt;86.90&lt;/strong&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;strong&gt;78.25&lt;/strong&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;strong&gt;83.38&lt;/strong&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;a href=&quot;http://aclweb.org/anthology/P18-2058&quot;&gt;ACL-18&lt;/a&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;He et al. (2018) ELMo&lt;br /&gt;&lt;em&gt;University of Washington&lt;/em&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;86.0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;76.1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;82.9&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;a href=&quot;http://aclweb.org/anthology/P17-1044&quot;&gt;ACL-17&lt;/a&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;He et al. (2017) GloVe+PoE&lt;br /&gt;&lt;em&gt;University of Wahsington, Facebook AI Research &amp;amp; Allen Institute for Artificial Intelligence&lt;/em&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;82.7&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;70.1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;78.4&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot;&gt;
      &lt;p&gt;Martha Palmer, Daniel Gildea, and Paul Kingsbury. 2005. The proposition bank: An annotated corpus of semantic roles. Computational Linguistics, 31(1):71-106. &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot;&gt;
      &lt;p&gt;Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. 1993. Building a large annotated corpus of english: the penn treebank. Computational Linguistics, 19(2):313{330. &lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:3&quot;&gt;
      &lt;p&gt;Xavier Carreras and Lluis Marquez. 2005. Introduction to the conll-2005 shared task: Semantic role labeling. In CoNLL. &lt;a href=&quot;#fnref:3&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:4&quot;&gt;
      &lt;p&gt;Sameer Pradhan, Alessandro Moschitti, Nianwen Xue, Hwee Tou Ng, Anders Bjorkelund, Olga Uryupina, Yuchen Zhang, and Zhi Zhong. 2013. Towards robust linguistic analysis using ontonotes. In CoNLL. &lt;a href=&quot;#fnref:4&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name>Zi Lin</name></author><category term="NLP Tasks" /><summary type="html">Introduction Semantic Role Labeling (SRL), sometimes called shallow semantic parsing, is a process in natrual language processing that assigns semantic roles to constituents or their head words in a sentence according to their relationship to the predicates expressed in the sentence. Typical semantic roles can be divided into core arguments and adjuncts. The core arguments include Agent, Patient, Source, Goal, etc, While the adjunts include Location, Time, Manner, Cause, etc. For example, in the sentence I ate breakfast quickly in the car this morning because I was in a hurry, we can have the labeled sentence like this: [I]agent [eat]predicate [breakfast]patient [quickly]manner [in the car]location [this morning]time [because I was in a hurry]cause . Source There are mainly three sources for SRL, namely, PropBank, FrameNet and VerbNet. PropBank PropBank is a corpus in which the arguments of each predicate are annotated with their semantic roles in relation to the predicate 1. Currently, all the PropBank annotations are done on top of the phrase structure annotation of the Pen TreeBank 2. In addition to semantic role annotation, PropBank annotation requires the choice of a sense ID (also known as frameset or roleset ID) for each predicate. Thus, for each verb in every tree (representing the phrase structure of the corresponding sentence), PropBank has the instance that consists of the sense ID of the predicate (e.g. eat.01) and its arguments labeled with semantic roles. An important goal is to provide consistent argument labels across different syntactic realizations of the same verb, as in… [I]ARG0 [eat]predicate [breakfast]ARG1 [quickly]ARGM-MNR [in the car]ARGM-LOC [this morning]ARGM-TMP [because I was in a hurry]ARGM-CAU . The argument structure of each predicate is outlined in the PropBank frame file for that predicate. The frame file denotes the correspondences between numbered arguments and semantic roles, as this is somewhat unique for each predicate. Numbered arguments reflect either the arguments that are requied for the valency of a predicate (e.g., agent, patient, benefactive), or if not requied, those that occur with high-frequency in actual usage. Although numbered arguments correspond to slightly different semantic roles given the usage of each predicate, in general numbered arguments correspond to the following semantic roles: ARG0 agent ARG3 starting point, benefactive, attribute ARG1 patient ARG4 ending point ARG2 instrument, benefactive, attribute ARGM modifier Data CoNLL-2005: The CoNLL-2005 dataset takes section 2-21 of the Wall Street Journal (WSJ) corpus as training set, and section 24 as development set. The test set consist of section 23 of the WSJ corpus (in-domain test) as well as 3 section from the Brown corpus (out-of-domain test) 3. CoNLL-2012: The CoNLL-2012 dataset is extracted from the OntoNotes v5.0 corpus 4. The description and separation of training, development and test set can be found in Pardhan et al. (2013). Experiments &amp;amp; Results Note: All the statistics are obtained from the exact papers or papers citing the excact papers. Contact me if the statistics are not correct or you think your model should be listed as one of them. With Gold Predicate Of course, for each model, we can have different versions, e.g., using the word embeddings from Glove or ELMo, and different versions can lead to different results. Here I just list the results for one of the versions, most of which are single models without using any tricks. I will specify if there are tricks leveraged, e.g., LISA+D&amp;amp;M. You should look the original paper if you are intersted in the model, and check whether there are some other versions of the model yielding better results. Paper Model WSJ Brown CoNLL-2012 EMNLP-18 LISA+D&amp;amp;MUniversity of Massachusetts Amherst &amp;amp; Google AI Language 86.04 76.54 - NAACL-18 Peter et al. (2018)+ELMoAllen Institute for Artificial Intelligence &amp;amp; University of Washington - - 84.6 AAAI-18 Tan et al. (2018)Xiamen University &amp;amp; Tencent Technology Co. 84.8 74.1 82.7 ACL-18 He et al. (2018)University of Washington 83.9 73.7 82.1 ACL-17 He et al. (2017)University of Wahsington, Facebook AI Research &amp;amp; Allen Institute for Artificial Intelligence 83.1 72.1 81.7 EMNLP-17 Yang and Mitchell (2017)Carnegie Mellon University 81.9 72.0 - ACL-15 Zhou and Xu (2015)Baidu Research 82.8 69.4 81.1 EMNLP-15 FitzGerald at al. (2015)University of Washington &amp;amp; Google 80.3 72.2 80.1 TACL-15 Tackstrom et al. (2015)Google 79.9 71.3 79.4 CoNLL-13 Pradhan et al. (2013)Boston Childrens Hospital and Harvard Medical School, University of Trento, QCRI, Brandeis University, National University of Singapore &amp;amp; University of Stuttgart - - 77.5 CL-08 Toutanova et al. (2008)Microsoft Research, University of California berkeley &amp;amp; Stanford University 80.3 68.8 - CL-08 Punyakanok et al. (2008)BBN Technologies, University of Illinois at Urbana-Champaign &amp;amp; Microsoft Research 79.4 67.8 - Without Gold Predicate Paper Model WSJ Brown CoNLL-2012 EMNLP-18 LISA+ELMo+D&amp;amp;MUniversity of Massachusetts Amherst &amp;amp; Google AI Language 86.90 78.25 83.38 ACL-18 He et al. (2018) ELMoUniversity of Washington 86.0 76.1 82.9 ACL-17 He et al. (2017) GloVe+PoEUniversity of Wahsington, Facebook AI Research &amp;amp; Allen Institute for Artificial Intelligence 82.7 70.1 78.4 Martha Palmer, Daniel Gildea, and Paul Kingsbury. 2005. The proposition bank: An annotated corpus of semantic roles. Computational Linguistics, 31(1):71-106. &amp;#8617; Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. 1993. Building a large annotated corpus of english: the penn treebank. Computational Linguistics, 19(2):313{330. &amp;#8617; Xavier Carreras and Lluis Marquez. 2005. Introduction to the conll-2005 shared task: Semantic role labeling. In CoNLL. &amp;#8617; Sameer Pradhan, Alessandro Moschitti, Nianwen Xue, Hwee Tou Ng, Anders Bjorkelund, Olga Uryupina, Yuchen Zhang, and Zhi Zhong. 2013. Towards robust linguistic analysis using ontonotes. In CoNLL. &amp;#8617;</summary></entry><entry><title type="html">Brief Introduction to Abstract Meaning Representation</title><link href="http://localhost:4000/intro-amr/" rel="alternate" type="text/html" title="Brief Introduction to Abstract Meaning Representation" /><published>2018-04-13T00:00:00+08:00</published><updated>2018-04-13T00:00:00+08:00</updated><id>http://localhost:4000/intro-amr</id><content type="html" xml:base="http://localhost:4000/intro-amr/">&lt;p&gt;&lt;strong&gt;AMR (Abstract Meaning Representation)&lt;/strong&gt;, is a set of sentences paired with simple, readable semantic representations. Remember that we have talked about FrameNet, a well-known ontology for frame semantic representations. If you want to know more about AMR, click &lt;a href=&quot;http://zi-lin.com/pdf/intro-AMR_zilin.pdf&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;</content><author><name>Zi Lin</name></author><category term="Meaning Representation" /><summary type="html">AMR (Abstract Meaning Representation), is a set of sentences paired with simple, readable semantic representations. Remember that we have talked about FrameNet, a well-known ontology for frame semantic representations. If you want to know more about AMR, click here.</summary></entry><entry><title type="html">Brief Introduction to Ontologies</title><link href="http://localhost:4000/intro-ontology/" rel="alternate" type="text/html" title="Brief Introduction to Ontologies" /><published>2017-09-21T00:00:00+08:00</published><updated>2017-09-21T00:00:00+08:00</updated><id>http://localhost:4000/intro-ontology</id><content type="html" xml:base="http://localhost:4000/intro-ontology/">&lt;p&gt;Have you ever heard of WordNet, HowNet, FrameNet and ConceptNet? There are different kinds of ontologies towards lexicon, syntax and commonsense knowledge.&lt;/p&gt;

&lt;p&gt;I have made a slides helping you to learn something about them, click &lt;a href=&quot;http://zi-lin.com/pdf/intro-ontologies.pdf&quot;&gt;here&lt;/a&gt; if you are interested!&lt;/p&gt;</content><author><name>Zi Lin</name></author><category term="Meaning Representation" /><summary type="html">Have you ever heard of WordNet, HowNet, FrameNet and ConceptNet? There are different kinds of ontologies towards lexicon, syntax and commonsense knowledge.</summary></entry><entry><title type="html">Build a Simple C Compiler with a Parse Tree!</title><link href="http://localhost:4000/build-c-compiler/" rel="alternate" type="text/html" title="Build a Simple C Compiler with a Parse Tree!" /><published>2017-06-01T00:00:00+08:00</published><updated>2017-06-01T00:00:00+08:00</updated><id>http://localhost:4000/build-c-compiler</id><content type="html" xml:base="http://localhost:4000/build-c-compiler/">&lt;p&gt;Hi, there! I has completed my assignment for the course called Principles of Compilers eventually - building a simple C compiler. Additionally, I also wrote a Python draft to visualize the parse tree :) For more information about the process, please click &lt;a href=&quot;http://zi-lin.com/pdf/assignment_report.pdf&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;</content><author><name>Zi Lin</name></author><category term="Coursework" /><summary type="html">Hi, there! I has completed my assignment for the course called Principles of Compilers eventually - building a simple C compiler. Additionally, I also wrote a Python draft to visualize the parse tree :) For more information about the process, please click here.</summary></entry><entry><title type="html">Brief Introduction to Chinese Morphology</title><link href="http://localhost:4000/chinese-morphology/" rel="alternate" type="text/html" title="Brief Introduction to Chinese Morphology" /><published>2017-03-06T00:00:00+08:00</published><updated>2017-03-06T00:00:00+08:00</updated><id>http://localhost:4000/chinese-morphology</id><content type="html" xml:base="http://localhost:4000/chinese-morphology/">&lt;p&gt;This is the draft I wrote for my presentation for the course called &lt;strong&gt;English Lexicoiogy&lt;/strong&gt;, where I was asked to introduce something about Chinese morphology to student from College of Foreign Languages.&lt;/p&gt;

&lt;p&gt;Chinese morphology, or rather, Chinese word-formation is quite different from English for it has less derivation. Also, parataxis plays a vital role in the process of word building. You can click &lt;a href=&quot;http://zi-lin.com/pdf/introduction-chinese-morphology.pdf&quot;&gt;here&lt;/a&gt; to get access to my draft.&lt;/p&gt;</content><author><name>Zi Lin</name></author><category term="Linguistics" /><summary type="html">This is the draft I wrote for my presentation for the course called English Lexicoiogy, where I was asked to introduce something about Chinese morphology to student from College of Foreign Languages.</summary></entry></feed>