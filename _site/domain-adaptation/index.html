<!DOCTYPE html>
<!--
    So Simple Jekyll Theme 3.1.1
    Copyright 2013-2018 Michael Rose - mademistakes.com | @mmistakes
    Free for personal and commercial use under the MIT license
    https://github.com/mmistakes/so-simple-theme/blob/master/LICENSE
-->
<html lang="en-US" class="no-js">
  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  

  
    
    <title>Zi Lin</title>
  

  <script>
    /* Cut the mustard */
    if ( 'querySelector' in document && 'addEventListener' in window ) {
      document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + 'js';
    }
  </script>

  <link rel="stylesheet" href="/assets/css/main.css">
  
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:400,400i,700,700i|Lora:400,400i,700,700i">
  <link rel="alternate" type="application/atom+xml" title="Zi Lin" href="/atom.xml">
<!-- start custom head snippets -->

<!-- insert favicons. use http://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

</head>

  <body class="layout--post  domain-adaptation-for-syntactic-analysis">
    <nav class="skip-links">
  <h2 class="screen-reader-text">Skip links</h2>
  <ul>
    <li><a href="#primary-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    
  <div class="navigation-wrapper">
    <a href="#menu-toggle" id="menu-toggle">Menu</a>
    <nav id="primary-nav" class="site-nav animated drop">
      <ul><li><a href="/">Home</a></li><li><a href="/blog/">Blog</a></li><li><a href="/tag/tag/">Tags</a></li><li><a href="/search/">Search</a></li></ul>
    </nav>
  </div><!-- /.navigation-wrapper -->


    <header class="masthead">
  <div class="wrap">
    
      <a href="/" class="site-logo" rel="home" title="Zi Lin">
        <img src="/images/zilin.jpg" class="site-logo-img animated fadeInDown" alt="Zi Lin">
      </a>
    
    
      <h1 class="site-title animated fadeIn"><a href="/">Zi Lin</a></h1>
      <p class="site-description animated fadeIn" itemprop="description">[firstname].[lastname]@pku.edu.cn</p>
    
  </div>
</header><!-- /.masthead -->


    <main id="main" class="main-content" aria-label="Content">
  <article class="h-entry">
    

    <div class="page-wrapper">
      <header class="page-header">
        
        
          <h1 id="page-title" class="page-title p-name">Domain Adaptation for Syntactic Analysis
</h1>
        
      </header>

      <div class="page-sidebar">
        <div class="page-author h-card p-author"><img src="/images/zilin.jpg" class="author-avatar u-photo" alt="Zi Lin"><div class="author-info"><div class="author-name">
        <em>by</em> <span class="p-name">Zi Lin</span>
      </div><ul class="author-links"><li class="author-link">
            <a class="u-url" rel="me" href="https://twitter.com/zilin97"><i class="fab fa-twitter-square fa-lg" title="Twitter"></i></a>
          </li><li class="author-link">
            <a class="u-url" rel="me" href="https://github.com/zi-lin"><i class="fab fa-github-square fa-lg" title="Github"></i></a>
          </li></ul>
    <time class="page-date dt-published" datetime="2019-05-01T00:00:00+08:00"><a class="u-url" href="">May 1, 2019</a>
</time>

  </div>
</div>

        

        
  <h3 class="page-taxonomies-title">Tags</h3>
  <ul class="page-taxonomies"><li class="page-taxonomy">NLP Tasks</li>
  </ul>


      </div>

      <div class="page-content">
        <div class="e-content">
          <h1 id="introduction">Introduction</h1>
<p>Current NLP systems tend to perform well only on their training domain and nearby genres, while the performances often degrade on the data drawn from different domains.</p>

<p>Recently, many endeavors have been made to explore and solve the problems of domain adaptation, ranging from creating challenging datasets to building algorithm that can avoid overfitting on the training data or transfer to the new domains. Here we will discuss several datasets build for the out-of-domain NLP task of syntactic analysis as well as some ideas on how to address the problem of domain adaptation. Note that in the literature, many definitions of the source domain and the target domain are proposed, here domain adaptation refers to the domains within the same NLP task and language (mainly English), but for other genres and domains such as emails, web forums and biomedical papers in terms of the dominant source domain of the Penn Treebank of Wall Street Journal (WSJ, financial news).</p>

<h1 id="datasets">Datasets</h1>

<ul>
  <li>
    <p><strong><a href="https://catalog.ldc.upenn.edu/LDC2009T26">Brown, ATIS &amp; Switchboard of Treebank-3</a></strong>: The Treebank-3 released by LDC also contains different domains other than WSJ: (1) the Brown Corpus (domain of literature), which has been completely retagged using the Penn Treebank tag set; (2) ATIS, the data from Department of Energy abstracts; (3) Switchboard, a collection of spontaneous telephone conversations between previously unacquainted speakers of American English on a variety of topics chosen from a pre-determined list.</p>
  </li>
  <li>
    <p><strong><a href="https://catalog.ldc.upenn.edu/LDC2012T13">English Web Treebank (EWT)</a></strong>: English Web Treebank was developed by the Linguistic Data Consortium (LDC) with funding from Google Inc. It contains 254,830 word-level tokens and 16,624 sentence-level tokens of webtext in 1174 files annotated for sentence- and word-level <strong>tokenization,</strong> <strong>part-of-speech</strong>, and <strong>syntactic structure</strong>. The data is roughly evenly divided across five genres: weblogs, newsgroups, email, reviews, and question-answers. The files were manually annotated following the sentence-level tokenization guidelines for web text and the word-level tokenization guidelines developed for English treebanks.</p>
  </li>
  <li>
    <p><strong><a href="https://github.com/Oneplus/Tweebank">Tweebank</a></strong>: Tweebank v2 is a collection of English tweets annotated in <strong>Universal Dependencies</strong> that can be exploited for the training of NLP systems to enhance their performance on social media texts. It is built on the original data of Tweebank v1 (840 unique tweets, 639/201 for training/test set), along with an additional 210 tweets sampled from the <strong>POS-tagged</strong> dataset of Gimpel et al. (2011) <sup id="fnref:1"><a href="#fn:1" class="footnote">1</a></sup> and 2,500 tweets sampled from the Twitter stream from Feb. 2016 to Jul. 2016. The latter data source consists of 147.4M English tweets. In the same way as Kong et al. (2011)<sup id="fnref:2"><a href="#fn:2" class="footnote">2</a></sup>, reference unit is always the tweet in its entirety – which may thus consist of multiple sentences – not the sentence alone.</p>
  </li>
  <li>
    <p><strong><a href="https://www.computing.dcu.ie/~jjudge/qtreebank/">QuestionBank</a></strong>: a corpus of 4000 <strong>parse-annotated</strong> questions for use in training parsers employed in QA and evaluation of question parsing. In 2011, Chris Manning spent some time improving the parses in QuestionBank, and the results of that work appear <a href="https://nlp.stanford.edu/data/QuestionBank-Stanford.shtml">here</a>. The corrected version was released as a script that maps the original to the corrected version.</p>
  </li>
  <li>
    <p><strong><a href="https://catalog.ldc.upenn.edu/LDC2012T02">English Translation Treebank (ETT)</a></strong>: English Translation Treebank consists of 599 distinct newswire stories from the Lebanese publication An Nahar translated from Arabic to English and annotated for <strong>part-of-speech</strong> and <strong>syntactic structure</strong>. This corpus is part of an effort at LDC to produce parallel Arabic and English treebanks.</p>
  </li>
  <li>
    <p><strong><a href="https://catalog.ldc.upenn.edu/LDC2008T21">PennBioIE Oncology</a></strong>: The PennBioIE Oncology Corpus consists of 1414 <a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi">PubMed</a> abstracts on cancer, concentrating on molecular genetics, and comprising approximately 327,000 words of biomedical text, tokenized and annotated for paragraph, sentence, <strong>part of speech</strong>, and 24 types of <strong>biomedical named entities</strong> in five categories of interest. 318 of the abstracts have also been <strong>syntactically</strong> annotated. All of the annotation was based on <a href="http://www.cis.upenn.edu/~treebank/">Penn Treebank II</a> standards, with some modifications for special characteristics of the biomedical text.</p>
  </li>
  <li>
    <p><strong><a href="http://www.geniaproject.org/genia-corpus">GENIA corpus</a></strong>: The corpus contains 1,999 Medline abstracts, selected using a <a href="http://pubmed.com/">PubMed</a> query for the three MeSH terms “human”, “blood cells”, and “transcription factors”. The corpus has been annotated with various levels of linguistic and semantic information, including: <strong>part-of-speech annotation</strong>, <strong>constituency syntactic annotation</strong>, term annotation, event annotation, relation annotation, and <strong>coreference annotation</strong>.</p>
  </li>
</ul>

<table>
  <thead>
    <tr>
      <th>Dataset</th>
      <th>Style</th>
      <th style="text-align: center">POS</th>
      <th style="text-align: center">NER</th>
      <th style="text-align: center">Coref</th>
      <th style="text-align: right">#Token</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Brown</td>
      <td>PTB</td>
      <td style="text-align: center">✔</td>
      <td style="text-align: center"> </td>
      <td style="text-align: center"> </td>
      <td style="text-align: right">486,140</td>
    </tr>
    <tr>
      <td>ATIS</td>
      <td>PTB</td>
      <td style="text-align: center">✔</td>
      <td style="text-align: center"> </td>
      <td style="text-align: center"> </td>
      <td style="text-align: right">-</td>
    </tr>
    <tr>
      <td>Switchboard</td>
      <td>PTB</td>
      <td style="text-align: center">✔</td>
      <td style="text-align: center"> </td>
      <td style="text-align: center"> </td>
      <td style="text-align: right">-</td>
    </tr>
    <tr>
      <td>EWT</td>
      <td>PTB</td>
      <td style="text-align: center">✔</td>
      <td style="text-align: center"> </td>
      <td style="text-align: center"> </td>
      <td style="text-align: right">254,830</td>
    </tr>
    <tr>
      <td>Tweebank</td>
      <td>UD</td>
      <td style="text-align: center">✔</td>
      <td style="text-align: center"> </td>
      <td style="text-align: center"> </td>
      <td style="text-align: right">55,427</td>
    </tr>
    <tr>
      <td>QuestionBank</td>
      <td>PTB</td>
      <td style="text-align: center">✔</td>
      <td style="text-align: center"> </td>
      <td style="text-align: center"> </td>
      <td style="text-align: right">4,000</td>
    </tr>
    <tr>
      <td>ETT</td>
      <td>PTB</td>
      <td style="text-align: center">✔</td>
      <td style="text-align: center"> </td>
      <td style="text-align: center"> </td>
      <td style="text-align: right">461,489</td>
    </tr>
    <tr>
      <td>PennBio</td>
      <td>PTB</td>
      <td style="text-align: center">✔</td>
      <td style="text-align: center">✔</td>
      <td style="text-align: center"> </td>
      <td style="text-align: right">327,000</td>
    </tr>
    <tr>
      <td>GENIA</td>
      <td>PTB</td>
      <td style="text-align: center">✔</td>
      <td style="text-align: center"> </td>
      <td style="text-align: center">✔</td>
      <td style="text-align: right">468,793</td>
    </tr>
  </tbody>
</table>

<h1 id="tagging--parsing">Tagging &amp; Parsing</h1>

<p>Domain adaptation has been recognized as a major NLP problem for over a decade. In particular, bridging the performance gap between in-domain and out-of-domain for tasks such as POS-tagging and parsing has received considerable attention. Here we will investigate some techniques for out-of-domain parsing.</p>

<h2 id="semi-supervised-parser-domain-adaptation-with-self-training">Semi-supervised parser domain adaptation with self-training</h2>

<p>Because treebanks are expensive to create, while plain text in most domains is easily obtainable, semi-supervised approaches to parser domain adaptation are a particularly attractive solution to the domain portability problem. This usually involves a manually annotated training set (a treebank), and a larger set of unlabeled data (plain text).</p>

<p>Many work in unsupervised domain adaptation for state-of-the-art parsers has achieved accuracy levels on out-of-domain text that is comparable to that achieved with domain-specific training data. This is done in a self-training setting, where a parser trained on a treebank (in a seed domain) is used to parse a large amount of unlabeled data in the target domain (assigning only one parse per sentence). The automatically parsed corpus is then used as additional training data for the parser. Specifically, their highlights are:</p>

<ul>
  <li>
    <p><strong>Reranking</strong>: McClosky et al. (2006b)<sup id="fnref:3"><a href="#fn:3" class="footnote">3</a></sup> presented the most successful semi-supervised approach to date for adaptation of a WSJ-trained parser to Brown data containing several genres of text. Their approach involves the use of a first-stage n-best parser and a reranker, which together produce parses for the unlabeled dataset. The automatically parsed in-domain corpus is then used as additional training material. In their previous work<sup id="fnref:5"><a href="#fn:5" class="footnote">4</a></sup>, McClosky et al. (2006a) argue that the use of a reranker is an important factor in the success of their approach, which is a layered model: the baser layer is a generative statistical PCFG parser that creates a ranked list of k parses (say, 50), and the second layer is a reranker that reorders these parses using more detailed features.</p>
  </li>
  <li>
    <p><strong>Small seed</strong>: Reichart and Rappoport (2007)<sup id="fnref:4"><a href="#fn:4" class="footnote">5</a></sup> used self-training to enhance the performance of a generative statistical PCFG parser for both the in-domain and the parser adaptation scenarios. The difference between their parser and McClosky et al.’s: [1] the 1-best list of a base parser can be used as a self-training material for the base parser itself; [2] the self-training can be done when the seed is small. They achieved this by identifying the self-training protocols: the self-training set contains several thousand sentences. A parser trained with a small seed set parses the self-training set, and then the <em>whole</em> automatically annotated self-training set is combined with the manually annotated seed set to retrain the parser.</p>
  </li>
  <li>
    <p><strong>Confidence-based</strong>: Sagae and Tsujii (2007)<sup id="fnref:6"><a href="#fn:6" class="footnote">6</a></sup> improved the out-of-domain accuracy of a dependency parser trained on the entire WSJ training set (40k sentences) by using unlabeled data in the same domain as the out-of-domain test data (biomedical text). They use agreement between different parsers to estimate the quality of automatically generated training instances and selected only sentences with high estimated accuracy.</p>

    <p>Yu et al. (2015)<sup id="fnref:9"><a href="#fn:9" class="footnote">7</a></sup> used confidence-based methods to select additional training samples, where they compared two confidence-based methods: [1] using the parse score of the employed parser to measure the confidence into a parse tree; [2] calculating the score differences between the best tree and alternative trees.</p>
  </li>
  <li>
    <p><strong>Simple:</strong> Sagae (2010)<sup id="fnref:7"><a href="#fn:7" class="footnote">8</a></sup> proposed a simple self-training framework for domain adaptation to avoid the settings as in previous work (without reranking, other means of training instance selection, or estimation of parse quality). The work investigated the contribution of the reranker for a constituency parser, and suggested that constituency parsers without a reranker can achieve significant improvements on the target domain but the results are still higher when a reranker is used.</p>
  </li>
  <li>
    <p><strong>Useful info from target domain</strong>: Chen et al. (2008)<sup id="fnref:8"><a href="#fn:8" class="footnote">9</a></sup> proposed a model that learns reliable information on shorter dependencies from unlabeled target domain data to help parse longer distance words. The rationale for this procedure is the observation that short dependency edges show a higher accuracy than longer edges.</p>
  </li>
</ul>

<h2 id="co-training">Co-training</h2>

<h2 id="up-training">Up-training</h2>

<h2 id="learning-from-partial-annotation">Learning from partial annotation</h2>

<div class="footnotes">
  <ol>
    <li id="fn:1">
      <p>Kevin Gimpel, Nathan Schneider, Brendan O’Connor, Dipanjan Das, Daniel Mills, Jacob Eisenstein, Michael Heilman, Dani Yogatama, Jeffrey Flanigan, and Noah A. Smith. 2011. Part-of-speech tagging for twitter: Annotation, features, and experiments. In Proc. of ACL. <a href="#fnref:1" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:2">
      <p>Lingpeng Kong, Nathan Schneider, Swabha Swayamdipta, Archna Bhatia, Chris Dyer, and Noah A. Smith. 2014. A Dependency Parser for Tweets. In Proc. of EMNLP. <a href="#fnref:2" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:3">
      <p>David McClosky, Eugene Charniak, and Mark Johnson. 2006b. Reranking and self-training for parser adaptation. In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics, pages 337–344. Association for Computational Linguistics. <a href="#fnref:3" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:5">
      <p>David McClosky, Eugene Charniak, and Mark Johnson. 2006b. Reranking and self-training for parser adaptation. In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics, pages 337–344. Association for Computational Linguistics. <a href="#fnref:5" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:4">
      <p>Roi Reichart and Ari Rappoport. 2007. Self-training for enhancement and domain adaptation of statistical parsers trained on small datasets. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics (ACL). Pages 616-623. Prague, Czech Republic. <a href="#fnref:4" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:6">
      <p>Kenji Sagae and Jun’ichi Tsujii. 2007. Multilingual dependency parsing and domain adaptation with data-driven LR models and parser ensembles. In Proceedings of the CoNLL 2007 shared task. Prague, Czech Republic. <a href="#fnref:6" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:9">
      <p>Juntao Yu, Mohab Elkaref, and Bernd Bohnet. 2015. Domain adaptation for dependency parsing via selftraining. In IWPT. <a href="#fnref:9" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:7">
      <p>Kenji Sagae. 2010. Self-training without reranking for parser domain adaptation and its impact on semantic role labeling. In Proceedings of the 2010 Workshop on Domain Adaptation for Natural Language Processing, pages 37–44. Association for Computational Linguistics. <a href="#fnref:7" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:8">
      <p>Wenliang Chen, Youzheng Wu, and Hitoshi Isahara. 2008. Learning reliable information for dependency parsing adaptation. In Proceedings of the 22nd International Conference on Computational Linguistics-Volume 1, pages 113–120. Association for Computational Linguistics. <a href="#fnref:8" class="reversefootnote">&#8617;</a></p>
    </li>
  </ol>
</div>

        </div>

        
          <div class="page-share">
  <a href="https://www.facebook.com/sharer/sharer.php?u=http%3A%2F%2Flocalhost%3A4000%2Fdomain-adaptation%2F" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" class="btn btn--facebook btn--small"><i class="fab fa-fw fa-facebook" aria-hidden="true"></i> <span>Share</span></a>
  <a href="https://twitter.com/intent/tweet?text=Domain+Adaptation+for+Syntactic+Analysis%20http%3A%2F%2Flocalhost%3A4000%2Fdomain-adaptation%2F" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" class="btn btn--twitter btn--small"><i class="fab fa-fw fa-twitter" aria-hidden="true"></i> <span>Tweet</span></a>
  <a href="https://www.linkedin.com/shareArticle?mini=true&url=http%3A%2F%2Flocalhost%3A4000%2Fdomain-adaptation%2F" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" class="btn btn--linkedin btn--small"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i> <span>LinkedIn</span></a>
  <a href="https://reddit.com/submit?title=Domain+Adaptation+for+Syntactic+Analysis&url=http%3A%2F%2Flocalhost%3A4000%2Fdomain-adaptation%2F" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" class="btn btn--reddit btn--small"><i class="fab fa-fw fa-reddit" aria-hidden="true"></i> <span>Reddit</span></a>
</div>

        

        

        <nav class="page-pagination" role="navigation">
  
    <a class="page-previous" href="/constituent-to-dependency/">
      <h4 class="page-pagination-label">Previous</h4>
      <span class="page-pagination-title">
        <i class="fas fa-arrow-left"></i> Constituent-to-dependency Conversion

      </span>
    </a>
  

  
</nav>

      </div>
    </div>
  </article>
</main>


    <footer id="footer" class="site-footer">
  <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
<div class="social-icons"><a class="social-icon" href="https://github.com/zi-lin"><i class="fab fa-github-square fa-2x" title="GitHub"></i></a><a class="social-icon" href="https://www.facebook.com/suzzzylin"><i class="fab fa-facebook-square fa-2x" title="Facebook"></i></a><a class="social-icon" href="https://twitter.com/suzzzylin"><i class="fab fa-twitter-square fa-2x" title="Twitter"></i></a><a class="social-icon" href="https://instagram.com/suzzzylin"><i class="fab fa-instagram fa-2x" title="Instagram"></i></a><a class="social-icon" href="https://www.linkedin.com/in/zi-lin/"><i class="fab fa-linkedin fa-2x" title="Linkedin"></i></a></div><div class="copyright">
    
      <p>&copy; 2019 Zi Lin. Powered by <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://github.com/mmistakes/so-simple-theme" rel="nofollow">So Simple</a>.</p>
    
  </div>
</footer>

    <script src="https://code.jquery.com/jquery-3.3.1.min.js" integrity="sha256-FgpCb/KJQlLNfOu91ta32o/NMZxltwRo8QtmkMRdAu8=" crossorigin="anonymous"></script>
  <script src="/assets/js/main.min.js"></script>
  <script src="https://use.fontawesome.com/releases/v5.0.12/js/all.js"></script>


<!-- MathJax -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

    <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-129673678-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-129673678-1');
</script>
  </body>

</html>
